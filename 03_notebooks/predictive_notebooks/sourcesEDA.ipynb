{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d74b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 0) Veri yükle\n",
    "# --------------------------------------------------------\n",
    "DATA_DIR    = Path(\"../../01_data\")\n",
    "df = pd.read_parquet(DATA_DIR / \"predictive_model\" /\"df_auto_corpus_area_tech.parquet\")  # path'i gerekirse değiştir\n",
    "\n",
    "# Varsayım: aşağıdaki kolonlar var:\n",
    "#   - source_type  (paper / patent / ...)\n",
    "#   - auto_top8_pred\n",
    "#   - seed_top1_sim\n",
    "#   - seed_top2_sim\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 1) AREA + high-confidence filtre tanımı\n",
    "# --------------------------------------------------------\n",
    "AREA = \"Cybersecurity_Safety_Governance\"\n",
    "\n",
    "# sadece paper + patent\n",
    "mask_pp = df[\"source_type\"].isin([\"paper\", \"patent\"])\n",
    "df[\"margin_pp\"] = df[\"seed_top1_sim\"]-df[\"seed_top2_sim\"]\n",
    "# high-confidence eşiklerini burada oynayabilirsin\n",
    "mask_conf = (\n",
    "    (df[\"seed_top1_sim\"] >= 0.60) & (df[\"margin_pp\"] >= 0.1)\n",
    ")\n",
    "\n",
    "mask_area = df[\"auto_focus_area\"] == AREA\n",
    "df_area = df[mask_pp & mask_conf & mask_area].copy()\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# EXTRA: AREA için en yüksek similarity'ye sahip ilk 5 satırı göster\n",
    "# --------------------------------------------------------\n",
    "top5 = (\n",
    "    df_area\n",
    "    .sort_values(\"seed_top1_sim\", ascending=False)\n",
    "    .head(5)[\n",
    "        [\"text\", \"seed_top1_sim\", \"auto_focus_area\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n=== AREA için en yüksek similarity'ye sahip ilk 5 doküman ===\")\n",
    "print(top5.to_string(index=False))\n",
    "print(\"============================================================\\n\")\n",
    "\n",
    "texts = df_area[\"text\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "\n",
    "print(f\"AREA: {AREA}\")\n",
    "print(\"Kullanılan doküman sayısı:\", len(texts))\n",
    "\n",
    "if len(texts) == 0:\n",
    "    raise ValueError(\"Bu filtrelerle hiç doküman yok, eşikleri/AREA'yı gevşet.\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 2) UNIGRAM\n",
    "# --------------------------------------------------------\n",
    "cv_uni = CountVectorizer(ngram_range=(1,1), min_df=3, stop_words=\"english\")\n",
    "X_uni = cv_uni.fit_transform(texts)\n",
    "unigram_counts = np.asarray(X_uni.sum(axis=0)).ravel()\n",
    "unigram_vocab = cv_uni.get_feature_names_out()\n",
    "\n",
    "df_uni = pd.DataFrame({\n",
    "    \"term\": unigram_vocab,\n",
    "    \"count\": unigram_counts\n",
    "}).sort_values(\"count\", ascending=False)\n",
    "\n",
    "uni_path = f\"word_frequency_unigram_{AREA}.csv\"\n",
    "df_uni.to_csv(uni_path, index=False)\n",
    "print(f\"✓ unigram kaydedildi -> {uni_path}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 3) BIGRAM\n",
    "# --------------------------------------------------------\n",
    "cv_bi = CountVectorizer(ngram_range=(2,2), min_df=3, stop_words=\"english\")\n",
    "X_bi = cv_bi.fit_transform(texts)\n",
    "bigram_counts = np.asarray(X_bi.sum(axis=0)).ravel()\n",
    "bigram_vocab = cv_bi.get_feature_names_out()\n",
    "\n",
    "df_bi = pd.DataFrame({\n",
    "    \"term\": bigram_vocab,\n",
    "    \"count\": bigram_counts\n",
    "}).sort_values(\"count\", ascending=False)\n",
    "\n",
    "bi_path = f\"word_frequency_bigram_{AREA}.csv\"\n",
    "df_bi.to_csv(bi_path, index=False)\n",
    "print(f\"✓ bigram kaydedildi -> {bi_path}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 4) TRIGRAM (istersen kapatabilirsin)\n",
    "# --------------------------------------------------------\n",
    "cv_tri = CountVectorizer(ngram_range=(3,3), min_df=3, stop_words=\"english\")\n",
    "X_tri = cv_tri.fit_transform(texts)\n",
    "trigram_counts = np.asarray(X_tri.sum(axis=0)).ravel()\n",
    "trigram_vocab = cv_tri.get_feature_names_out()\n",
    "\n",
    "df_tri = pd.DataFrame({\n",
    "    \"term\": trigram_vocab,\n",
    "    \"count\": trigram_counts\n",
    "}).sort_values(\"count\", ascending=False)\n",
    "\n",
    "tri_path = f\"word_frequency_trigram_{AREA}.csv\"\n",
    "df_tri.to_csv(tri_path, index=False)\n",
    "print(f\"✓ trigram kaydedildi -> {tri_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nTAMAM ✔ AREA bazlı CSV'ler üretildi:\")\n",
    "print(\"-\", uni_path)\n",
    "print(\"-\", bi_path)\n",
    "print(\"-\", tri_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
