{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5c14953",
   "metadata": {},
   "source": [
    "## Pre-processing and building RAG Retrieval Prototype\n",
    "\n",
    "#### What this script does:\n",
    "- Loads processed text files\n",
    "- Prepares documents for embedding by dividing into chunks \n",
    "- Creates embeddings using TF-IDF\n",
    "- Saves vector index in 04_models/vector_index/\n",
    "- Loads TF-IDF retriever from rag_componets/retriever.py\n",
    "- Tests retriever with some basic automotive-focused questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "611a6fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import importlib.util\n",
    "import sys\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b400ee47",
   "metadata": {},
   "source": [
    "### Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e09ac",
   "metadata": {},
   "source": [
    "1. Load reports and divide into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "471bcca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading research papers...\n",
      "Loaded 61 chunks from enhanced_drift_aware_computer_vision_achitecture_for_autonomous_driving.txt\n",
      "Loaded 102 chunks from Gen_AI_in_automotive_applications_challenges_and_opportunities_with_a_case_study_on_in-vehicle_experience.txt\n",
      "Loaded 120 chunks from leveraging_vision_language_models_for_visual_grounding_and_analysis_of_automative_UI.txt\n",
      "Loaded 11408 chunks from automotive_papers_processed.txt\n",
      "Loaded 69 chunks from automating_automative_software_development_a_synergy_of_generative_AI_and_formal_methods.txt\n",
      "Loaded 137 chunks from automotive-software-and-electronics-2030-full-report.txt\n",
      "Loaded 102 chunks from AI_agents_in_engineering_design_a_multiagent_framework_for_aesthetic_and_aerodynamic_car_design.txt\n",
      "Loaded 87 chunks from a_benchmark_framework_for_AI_models_in_automative_aerodynamics.txt\n",
      "Loaded 227 chunks from generative_AI_for_autonomous_driving_a_review.txt\n",
      "Loaded 46 chunks from Embedded_acoustic_intelligence_for_automotive_systems.txt\n",
      "Loaded 107 chunks from drive_disfluency-rich_synthetic_dialog_data_generation_framework_for_intelligent_vehicle_environments.txt\n",
      "Loading patents data...\n",
      "Loaded 4198 chunks from automotive_patents_processed.txt\n",
      "Loading tech reports...\n",
      "Loaded 403 chunks from mckinsey_tech_trends_2025.txt\n",
      "Loaded 189 chunks from wef_emerging_tech_2025.txt\n",
      "Loaded 95 chunks from bcg_ai_value_2025.txt\n",
      "Loading startups data...\n",
      "Loaded 1350 chunks from autotechinsight_startups_processed.txt\n",
      "Loaded 16 chunks from seedtable_startups_processed.txt\n",
      "\n",
      "Summary:\n",
      "- Research papers: 12466 chunks\n",
      "- Patents data: 4198 chunks\n",
      "- Tech reports: 687 chunks\n",
      "- Startups data: 1366 chunks\n",
      "Total chunks created: 18717\n"
     ]
    }
   ],
   "source": [
    "# Define paths where processed text files are found \n",
    "data_path = \"../../01_data/rag_automotive_tech/processed\"\n",
    "papers_path = os.path.join(data_path, \"automotive_papers\") # Added journal abstracts file\n",
    "patents_path = os.path.join(data_path, \"automotive_tech_patents\") # Added patent file\n",
    "reports_path = os.path.join(data_path, \"tech_reports\")\n",
    "startups_path = os.path.join(data_path, \"startups\")  # Added startups files\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Function for chunking documents from folders\n",
    "def load_and_chunk_documents(folder_path, doc_type):\n",
    "    \"\"\"Load and chunk documents from a specific folder\"\"\"\n",
    "    chunks = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                loader = TextLoader(file_path, encoding='utf-8')\n",
    "                documents = loader.load()\n",
    "                \n",
    "                for doc in documents:\n",
    "                    doc.metadata.update({\n",
    "                        'source': filename,\n",
    "                        'doc_type': doc_type,\n",
    "                        'file_path': file_path\n",
    "                    })\n",
    "                \n",
    "                doc_chunks = text_splitter.split_documents(documents)\n",
    "                chunks.extend(doc_chunks)\n",
    "                print(f\"Loaded {len(doc_chunks)} chunks from {filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Load and chunk all documents\n",
    "print(\"Loading research papers...\")\n",
    "papers_chunks = load_and_chunk_documents(papers_path, \"research_paper\")\n",
    "\n",
    "print(\"Loading patents data...\")\n",
    "patents_chunks = load_and_chunk_documents(patents_path, \"patents_data\")\n",
    "\n",
    "print(\"Loading tech reports...\")\n",
    "reports_chunks = load_and_chunk_documents(reports_path, \"tech_report\")\n",
    "\n",
    "print(\"Loading startups data...\")\n",
    "startups_chunks = load_and_chunk_documents(startups_path, \"startups\")\n",
    "\n",
    "# Combine all chunks\n",
    "all_chunks = papers_chunks + patents_chunks + reports_chunks + startups_chunks\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"- Research papers: {len(papers_chunks)} chunks\")\n",
    "print(f\"- Patents data: {len(patents_chunks)} chunks\")\n",
    "print(f\"- Tech reports: {len(reports_chunks)} chunks\")\n",
    "print(f\"- Startups data: {len(startups_chunks)} chunks\")\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b3eed3",
   "metadata": {},
   "source": [
    "2. Create embeddings using FAISS and save Vector Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe142c9",
   "metadata": {},
   "source": [
    "Switched from TF-IDF = Bag-of-words, no semantic understanding to \n",
    "Word2Vec/Transformers = Semantic understanding, contextual meaning and FAISS (Chroma DB had issues, always corrupted/damaged - see below) \n",
    "\n",
    "Model: BAAI/bge-small-en\n",
    "State-of-the-art for retrieval\n",
    "Slightly larger but excellent\n",
    "\n",
    "Used FastEmbed for good quality and loe memory (10-50x faster than regular sentence transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07fe7d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Removing existing directory: ../../04_models/vector_index\n",
      "âœ“ Removed\n",
      "âœ“ Created fresh directory: ../../04_models/vector_index\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# NUCLEAR CLEANUP - Remove everything\n",
    "vector_index_path = \"../../04_models/vector_index\"\n",
    "\n",
    "if os.path.exists(vector_index_path):\n",
    "    print(f\"âš ï¸ Removing existing directory: {vector_index_path}\")\n",
    "    shutil.rmtree(vector_index_path)\n",
    "    print(\"âœ“ Removed\")\n",
    "\n",
    "# Create fresh\n",
    "os.makedirs(vector_index_path, exist_ok=True)\n",
    "print(f\"âœ“ Created fresh directory: {vector_index_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6ca1406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Creating embeddings for 18717 chunks\n",
      "ğŸ§¹ Cleaning ../../04_models/vector_index\n",
      "ğŸ“ Fresh directory created\n",
      "ğŸ”® Loading embedding model...\n",
      "âœ¨ Creating embeddings...\n",
      "âœ“ Created 18717 embeddings\n",
      "\n",
      "ğŸ› ï¸ Initializing ChromaDB...\n",
      "âŒ Method A failed: Database error: error returned from database: (code: 1) no such table: tenants\n",
      "Trying alternative method...\n",
      "âŒ Both methods failed: An instance of Chroma already exists for ../../04_models/vector_index with different settings\n",
      "Trying in-memory approach...\n",
      "âœ“ Using in-memory collection (will not persist)\n",
      "\n",
      "ğŸ“¦ Adding documents...\n",
      "  âœ… Added 50/18717 chunks\n",
      "  âœ… Added 1050/18717 chunks\n",
      "  âœ… Added 2050/18717 chunks\n",
      "  âœ… Added 3050/18717 chunks\n",
      "  âœ… Added 4050/18717 chunks\n",
      "  âœ… Added 5050/18717 chunks\n",
      "  âœ… Added 6050/18717 chunks\n",
      "  âœ… Added 7050/18717 chunks\n",
      "  âœ… Added 8050/18717 chunks\n",
      "  âœ… Added 9050/18717 chunks\n",
      "  âœ… Added 10050/18717 chunks\n",
      "  âœ… Added 11050/18717 chunks\n",
      "  âœ… Added 12050/18717 chunks\n",
      "  âœ… Added 13050/18717 chunks\n",
      "  âœ… Added 14050/18717 chunks\n",
      "  âœ… Added 15050/18717 chunks\n",
      "  âœ… Added 16050/18717 chunks\n",
      "  âœ… Added 17050/18717 chunks\n",
      "  âœ… Added 18050/18717 chunks\n",
      "  âœ… Added 18717/18717 chunks\n",
      "\n",
      "ğŸ’¾ Saving metadata...\n",
      "âœ“ Metadata saved: ../../04_models/vector_index/chunks_metadata.pkl\n",
      "\n",
      "==================================================\n",
      "âœ… FINAL VERIFICATION\n",
      "==================================================\n",
      "\n",
      "ğŸ“Š Stats:\n",
      "Total chunks: 18717\n",
      "Embeddings created: 18717\n",
      "ChromaDB collection count: 18717\n",
      "\n",
      "ğŸ§ª Test query: 'automotive'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manueltimowolf/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79.3M/79.3M [00:25<00:00, 3.21MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Query successful! Found 2 results\n",
      "First result preview: RESEARCH PAPER #919:\n",
      "  Title: Mapping the Landscape of Romanian Automotive Research: A Bibliometric ...\n",
      "\n",
      "ğŸ“ Directory contents of ../../04_models/vector_index:\n",
      "  - chunks_metadata.pkl (11,772,358 bytes)\n",
      "\n",
      "==================================================\n",
      "ğŸ‰ PROCESS COMPLETE!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== ULTRA-SIMPLE CHROMADB CREATION =====\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from fastembed import TextEmbedding\n",
    "import chromadb\n",
    "import numpy as np\n",
    "\n",
    "print(f\"ğŸš€ Creating embeddings for {len(all_chunks)} chunks\")\n",
    "\n",
    "# 1. EXTRACT TEXTS\n",
    "texts = [chunk.page_content for chunk in all_chunks]\n",
    "\n",
    "# 2. SETUP PATH (CORRECT!)\n",
    "vector_index_path = \"../../04_models/vector_index\"\n",
    "\n",
    "# 3. CLEANUP\n",
    "import shutil\n",
    "if os.path.exists(vector_index_path):\n",
    "    print(f\"ğŸ§¹ Cleaning {vector_index_path}\")\n",
    "    shutil.rmtree(vector_index_path)\n",
    "os.makedirs(vector_index_path, exist_ok=True)\n",
    "print(f\"ğŸ“ Fresh directory created\")\n",
    "\n",
    "# 4. CREATE EMBEDDINGS\n",
    "print(\"ğŸ”® Loading embedding model...\")\n",
    "model = TextEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"âœ¨ Creating embeddings...\")\n",
    "embeddings = list(model.embed(texts))\n",
    "print(f\"âœ“ Created {len(embeddings)} embeddings\")\n",
    "\n",
    "# 5. INIT CHROMADB (ULTRA SIMPLE)\n",
    "print(\"\\nğŸ› ï¸ Initializing ChromaDB...\")\n",
    "\n",
    "# Method A: Try simplest approach first\n",
    "try:\n",
    "    client = chromadb.PersistentClient(path=vector_index_path)\n",
    "    print(\"âœ“ Client created\")\n",
    "    \n",
    "    # Delete if exists\n",
    "    try:\n",
    "        client.delete_collection(name=\"documents\")\n",
    "        print(\"âœ“ Cleaned old collection\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Create collection\n",
    "    collection = client.create_collection(name=\"documents\")\n",
    "    print(f\"âœ“ Collection created: {collection.name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Method A failed: {e}\")\n",
    "    \n",
    "    # Method B: Try with minimal settings\n",
    "    print(\"Trying alternative method...\")\n",
    "    try:\n",
    "        from chromadb.config import Settings\n",
    "        \n",
    "        client = chromadb.PersistentClient(\n",
    "            path=vector_index_path,\n",
    "            settings=Settings(\n",
    "                anonymized_telemetry=False,\n",
    "                chroma_server_grpc_port=None,\n",
    "                chroma_server_http_port=None,\n",
    "                chroma_server_ssl_enabled=False\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Force delete if exists\n",
    "        collections = client.list_collections()\n",
    "        for col in collections:\n",
    "            if col.name == \"documents\":\n",
    "                client.delete_collection(name=\"documents\")\n",
    "        \n",
    "        collection = client.create_collection(name=\"documents\")\n",
    "        print(f\"âœ“ Collection created (Method B): {collection.name}\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"âŒ Both methods failed: {e2}\")\n",
    "        print(\"Trying in-memory approach...\")\n",
    "        \n",
    "        # Method C: In-memory as last resort\n",
    "        client = chromadb.Client()\n",
    "        collection = client.create_collection(name=\"documents\")\n",
    "        print(\"âœ“ Using in-memory collection (will not persist)\")\n",
    "\n",
    "# 6. ADD DOCUMENTS\n",
    "print(\"\\nğŸ“¦ Adding documents...\")\n",
    "batch_size = 50\n",
    "total_chunks = len(texts)\n",
    "\n",
    "for i in range(0, total_chunks, batch_size):\n",
    "    end_idx = min(i + batch_size, total_chunks)\n",
    "    \n",
    "    batch_texts = texts[i:end_idx]\n",
    "    batch_embeddings = [emb.tolist() for emb in embeddings[i:end_idx]]\n",
    "    \n",
    "    batch_metadatas = []\n",
    "    batch_ids = []\n",
    "    \n",
    "    for j in range(len(batch_texts)):\n",
    "        chunk_idx = i + j\n",
    "        chunk = all_chunks[chunk_idx]\n",
    "        \n",
    "        batch_metadatas.append({\n",
    "            \"source\": str(chunk.metadata.get(\"source\", \"unknown\")),\n",
    "            \"doc_type\": str(chunk.metadata.get(\"doc_type\", \"document\")),\n",
    "            \"chunk_index\": chunk_idx\n",
    "        })\n",
    "        batch_ids.append(f\"chunk_{chunk_idx}\")\n",
    "    \n",
    "    try:\n",
    "        collection.add(\n",
    "            embeddings=batch_embeddings,\n",
    "            documents=batch_texts,\n",
    "            metadatas=batch_metadatas,\n",
    "            ids=batch_ids\n",
    "        )\n",
    "        \n",
    "        if (i // batch_size) % 20 == 0 or end_idx == total_chunks:\n",
    "            print(f\"  âœ… Added {end_idx}/{total_chunks} chunks\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸ Batch {i}-{end_idx} failed: {e}\")\n",
    "        # Try without embeddings\n",
    "        try:\n",
    "            collection.add(\n",
    "                documents=batch_texts,\n",
    "                metadatas=batch_metadatas,\n",
    "                ids=batch_ids\n",
    "            )\n",
    "            print(f\"  âœ… Added without embeddings\")\n",
    "        except Exception as e2:\n",
    "            print(f\"  âŒ Complete failure: {e2}\")\n",
    "\n",
    "# 7. SAVE METADATA\n",
    "print(\"\\nğŸ’¾ Saving metadata...\")\n",
    "try:\n",
    "    chunks_metadata = []\n",
    "    for i, chunk in enumerate(all_chunks):\n",
    "        chunks_metadata.append({\n",
    "            \"page_content\": chunk.page_content,\n",
    "            \"metadata\": chunk.metadata,\n",
    "            \"embedding_index\": i\n",
    "        })\n",
    "    \n",
    "    metadata_path = os.path.join(vector_index_path, \"chunks_metadata.pkl\")\n",
    "    with open(metadata_path, \"wb\") as f:\n",
    "        pickle.dump(chunks_metadata, f)\n",
    "    \n",
    "    print(f\"âœ“ Metadata saved: {metadata_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Metadata save failed: {e}\")\n",
    "\n",
    "# 8. VERIFY\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"âœ… FINAL VERIFICATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nğŸ“Š Stats:\")\n",
    "print(f\"Total chunks: {len(texts)}\")\n",
    "print(f\"Embeddings created: {len(embeddings)}\")\n",
    "\n",
    "try:\n",
    "    count = collection.count()\n",
    "    print(f\"ChromaDB collection count: {count}\")\n",
    "    \n",
    "    if count > 0:\n",
    "        # Test query\n",
    "        print(\"\\nğŸ§ª Test query: 'automotive'\")\n",
    "        results = collection.query(\n",
    "            query_texts=[\"automotive\"],\n",
    "            n_results=2\n",
    "        )\n",
    "        \n",
    "        if results and results['documents']:\n",
    "            print(f\"âœ“ Query successful! Found {len(results['documents'][0])} results\")\n",
    "            print(f\"First result preview: {results['documents'][0][0][:100]}...\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Query returned no results\")\n",
    "    else:\n",
    "        print(\"âŒ Collection is empty!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Verification failed: {e}\")\n",
    "\n",
    "# 9. CHECK DIRECTORY\n",
    "print(f\"\\nğŸ“ Directory contents of {vector_index_path}:\")\n",
    "try:\n",
    "    files = os.listdir(vector_index_path)\n",
    "    if files:\n",
    "        for file in files:\n",
    "            size = os.path.getsize(os.path.join(vector_index_path, file))\n",
    "            print(f\"  - {file} ({size:,} bytes)\")\n",
    "    else:\n",
    "        print(\"  (empty)\")\n",
    "except Exception as e:\n",
    "    print(f\"  Could not list: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ‰ PROCESS COMPLETE!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55eac2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting chromadb==0.4.22\n",
      "  Using cached chromadb-0.4.22-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting build>=1.0.3 (from chromadb==0.4.22)\n",
      "  Using cached build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting requests>=2.28 (from chromadb==0.4.22)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting pydantic>=1.9 (from chromadb==0.4.22)\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting chroma-hnswlib==0.7.3 (from chromadb==0.4.22)\n",
      "  Downloading chroma_hnswlib-0.7.3-cp39-cp39-macosx_10_9_x86_64.whl.metadata (252 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb==0.4.22)\n",
      "  Using cached fastapi-0.123.5-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb==0.4.22)\n",
      "  Using cached uvicorn-0.38.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting numpy>=1.22.5 (from chromadb==0.4.22)\n",
      "  Using cached numpy-2.0.2-cp39-cp39-macosx_14_0_x86_64.whl.metadata (60 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb==0.4.22)\n",
      "  Downloading posthog-6.9.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting typing-extensions>=4.5.0 (from chromadb==0.4.22)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pulsar-client>=3.1.0 (from chromadb==0.4.22)\n",
      "  Downloading pulsar_client-3.8.0-cp39-cp39-macosx_13_0_universal2.whl.metadata (1.0 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb==0.4.22)\n",
      "  Using cached onnxruntime-1.19.2-cp39-cp39-macosx_11_0_universal2.whl.metadata (4.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb==0.4.22)\n",
      "  Using cached opentelemetry_api-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb==0.4.22)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.39.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb==0.4.22)\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.60b0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb==0.4.22)\n",
      "  Using cached opentelemetry_sdk-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb==0.4.22)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb==0.4.22)\n",
      "  Using cached pypika-0.48.9-py2.py3-none-any.whl\n",
      "Collecting tqdm>=4.65.0 (from chromadb==0.4.22)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting overrides>=7.3.1 (from chromadb==0.4.22)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb==0.4.22)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb==0.4.22)\n",
      "  Using cached grpcio-1.76.0-cp39-cp39-macosx_11_0_universal2.whl.metadata (3.7 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb==0.4.22)\n",
      "  Using cached bcrypt-5.0.0-cp39-abi3-macosx_10_12_universal2.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb==0.4.22)\n",
      "  Using cached typer-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb==0.4.22)\n",
      "  Using cached kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tenacity>=8.2.3 (from chromadb==0.4.22)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting PyYAML>=6.0.0 (from chromadb==0.4.22)\n",
      "  Using cached pyyaml-6.0.3-cp39-cp39-macosx_10_13_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb==0.4.22)\n",
      "  Using cached mmh3-5.2.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (14 kB)\n",
      "Collecting packaging>=19.1 (from build>=1.0.3->chromadb==0.4.22)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb==0.4.22)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting importlib-metadata>=4.6 (from build>=1.0.3->chromadb==0.4.22)\n",
      "  Using cached importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting tomli>=1.1.0 (from build>=1.0.3->chromadb==0.4.22)\n",
      "  Using cached tomli-2.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting starlette<0.51.0,>=0.40.0 (from fastapi>=0.95.2->chromadb==0.4.22)\n",
      "  Downloading starlette-0.49.3-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting annotated-doc>=0.0.2 (from fastapi>=0.95.2->chromadb==0.4.22)\n",
      "  Using cached annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=1.9->chromadb==0.4.22)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic>=1.9->chromadb==0.4.22)\n",
      "  Using cached pydantic_core-2.41.5-cp39-cp39-macosx_10_12_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic>=1.9->chromadb==0.4.22)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting anyio<5,>=3.6.2 (from starlette<0.51.0,>=0.40.0->fastapi>=0.95.2->chromadb==0.4.22)\n",
      "  Downloading anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting exceptiongroup>=1.0.2 (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi>=0.95.2->chromadb==0.4.22)\n",
      "  Downloading exceptiongroup-1.3.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi>=0.95.2->chromadb==0.4.22)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata>=4.6->build>=1.0.3->chromadb==0.4.22)\n",
      "  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting certifi>=14.05.14 (from kubernetes>=28.1.0->chromadb==0.4.22)\n",
      "  Using cached certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting six>=1.9.0 (from kubernetes>=28.1.0->chromadb==0.4.22)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting python-dateutil>=2.5.3 (from kubernetes>=28.1.0->chromadb==0.4.22)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb==0.4.22)\n",
      "  Using cached google_auth-2.43.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb==0.4.22)\n",
      "  Using cached websocket_client-1.9.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb==0.4.22)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb==0.4.22)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb==0.4.22)\n",
      "  Using cached durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Collecting cachetools<7.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.22)\n",
      "  Using cached cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.22)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.22)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.22)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.4.22)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb==0.4.22)\n",
      "  Using cached flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb==0.4.22)\n",
      "  Using cached protobuf-6.33.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb==0.4.22)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting googleapis-common-protos~=1.57 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.22)\n",
      "  Using cached googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.39.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.22)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.39.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.39.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.22)\n",
      "  Using cached opentelemetry_proto-1.39.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.60b0 (from opentelemetry-sdk>=1.2.0->chromadb==0.4.22)\n",
      "  Using cached opentelemetry_semantic_conventions-0.60b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.60b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22)\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.60b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-instrumentation==0.60b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22)\n",
      "  Using cached opentelemetry_instrumentation-0.60b0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting opentelemetry-util-http==0.60b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22)\n",
      "  Using cached opentelemetry_util_http-0.60b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.60b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22)\n",
      "  Downloading wrapt-1.17.3-cp39-cp39-macosx_10_9_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.60b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22)\n",
      "  Using cached asgiref-3.11.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb==0.4.22)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting distro>=1.5.0 (from posthog>=2.4.0->chromadb==0.4.22)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.28->chromadb==0.4.22)\n",
      "  Using cached charset_normalizer-3.4.4-cp39-cp39-macosx_10_9_universal2.whl.metadata (37 kB)\n",
      "Collecting huggingface-hub<2.0,>=0.16.4 (from tokenizers>=0.13.2->chromadb==0.4.22)\n",
      "  Using cached huggingface_hub-1.1.7-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting filelock (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.22)\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.22)\n",
      "  Using cached fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.22)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-macosx_10_12_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.22)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.22)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.22)\n",
      "  Using cached typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.22)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.22)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting click>=8.0.0 (from typer>=0.9.0->chromadb==0.4.22)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich>=10.11.0 (from typer>=0.9.0->chromadb==0.4.22)\n",
      "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.22)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.22)\n",
      "  Using cached pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb==0.4.22)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb==0.4.22)\n",
      "  Using cached httptools-0.7.1-cp39-cp39-macosx_10_9_universal2.whl.metadata (3.5 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.22)\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb==0.4.22)\n",
      "  Using cached uvloop-0.22.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.22)\n",
      "  Using cached watchfiles-1.1.1-cp39-cp39-macosx_10_12_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb==0.4.22)\n",
      "  Using cached websockets-15.0.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.22)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib->kubernetes>=28.1.0->chromadb==0.4.22)\n",
      "  Using cached oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime>=1.14.1->chromadb==0.4.22)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached chromadb-0.4.22-py3-none-any.whl (509 kB)\n",
      "Downloading chroma_hnswlib-0.7.3-cp39-cp39-macosx_10_9_x86_64.whl (219 kB)\n",
      "Using cached bcrypt-5.0.0-cp39-abi3-macosx_10_12_universal2.whl (495 kB)\n",
      "Using cached build-1.3.0-py3-none-any.whl (23 kB)\n",
      "Using cached fastapi-0.123.5-py3-none-any.whl (111 kB)\n",
      "Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp39-cp39-macosx_10_12_x86_64.whl (2.1 MB)\n",
      "Downloading starlette-0.49.3-py3-none-any.whl (74 kB)\n",
      "Downloading anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "Using cached annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading exceptiongroup-1.3.1-py3-none-any.whl (16 kB)\n",
      "Using cached grpcio-1.76.0-cp39-cp39-macosx_11_0_universal2.whl (11.8 MB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Using cached kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Using cached durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Using cached google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
      "Using cached cachetools-6.2.2-py3-none-any.whl (11 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached mmh3-5.2.0-cp39-cp39-macosx_10_9_x86_64.whl (40 kB)\n",
      "Using cached numpy-2.0.2-cp39-cp39-macosx_14_0_x86_64.whl (6.9 MB)\n",
      "Using cached onnxruntime-1.19.2-cp39-cp39-macosx_11_0_universal2.whl (16.8 MB)\n",
      "Using cached opentelemetry_api-1.39.0-py3-none-any.whl (66 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.39.0-py3-none-any.whl (19 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.39.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_proto-1.39.0-py3-none-any.whl (72 kB)\n",
      "Using cached googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Using cached opentelemetry_sdk-1.39.0-py3-none-any.whl (132 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.60b0-py3-none-any.whl (219 kB)\n",
      "Using cached protobuf-6.33.1-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
      "Using cached opentelemetry_instrumentation_fastapi-0.60b0-py3-none-any.whl (13 kB)\n",
      "Using cached opentelemetry_instrumentation-0.60b0-py3-none-any.whl (33 kB)\n",
      "Using cached opentelemetry_instrumentation_asgi-0.60b0-py3-none-any.whl (16 kB)\n",
      "Using cached opentelemetry_util_http-0.60b0-py3-none-any.whl (8.7 kB)\n",
      "Using cached asgiref-3.11.0-py3-none-any.whl (24 kB)\n",
      "Downloading wrapt-1.17.3-cp39-cp39-macosx_10_9_x86_64.whl (38 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Downloading posthog-6.9.3-py3-none-any.whl (144 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp39-cp39-macosx_10_9_universal2.whl (209 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading pulsar_client-3.8.0-cp39-cp39-macosx_13_0_universal2.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pyyaml-6.0.3-cp39-cp39-macosx_10_13_x86_64.whl (184 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-macosx_10_12_x86_64.whl (3.1 MB)\n",
      "Using cached huggingface_hub-1.1.7-py3-none-any.whl (516 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-macosx_10_12_x86_64.whl (2.9 MB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached tomli-2.3.0-py3-none-any.whl (14 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typer-0.20.0-py3-none-any.whl (47 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Using cached pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
      "Using cached httptools-0.7.1-cp39-cp39-macosx_10_9_universal2.whl (206 kB)\n",
      "Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached uvloop-0.22.1-cp39-cp39-macosx_10_9_x86_64.whl (743 kB)\n",
      "Using cached watchfiles-1.1.1-cp39-cp39-macosx_10_12_x86_64.whl (409 kB)\n",
      "Using cached websocket_client-1.9.0-py3-none-any.whl (82 kB)\n",
      "Using cached websockets-15.0.1-cp39-cp39-macosx_10_9_x86_64.whl (173 kB)\n",
      "Using cached zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: pypika, mpmath, flatbuffers, durationpy, zipp, wrapt, websockets, websocket-client, uvloop, urllib3, typing-extensions, tqdm, tomli, tenacity, sympy, six, shellingham, PyYAML, python-dotenv, pyproject_hooks, pygments, pyasn1, protobuf, packaging, overrides, opentelemetry-util-http, oauthlib, numpy, mmh3, mdurl, idna, humanfriendly, httptools, hf-xet, h11, fsspec, filelock, distro, click, charset_normalizer, certifi, cachetools, bcrypt, backoff, annotated-types, annotated-doc, uvicorn, typing-inspection, typer-slim, rsa, requests, python-dateutil, pydantic-core, pyasn1-modules, pulsar-client, opentelemetry-proto, markdown-it-py, importlib-resources, importlib-metadata, httpcore, grpcio, googleapis-common-protos, exceptiongroup, coloredlogs, chroma-hnswlib, asgiref, rich, requests-oauthlib, pydantic, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, google-auth, build, anyio, watchfiles, typer, starlette, opentelemetry-semantic-conventions, kubernetes, httpx, opentelemetry-sdk, opentelemetry-instrumentation, huggingface-hub, fastapi, tokenizers, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
      "\u001b[2K  Attempting uninstall: pypika\n",
      "\u001b[2K    Found existing installation: PyPika 0.48.9\n",
      "\u001b[2K    Uninstalling PyPika-0.48.9:\n",
      "\u001b[2K      Successfully uninstalled PyPika-0.48.9\n",
      "\u001b[2K  Attempting uninstall: mpmath\n",
      "\u001b[2K    Found existing installation: mpmath 1.3.0\n",
      "\u001b[2K    Uninstalling mpmath-1.3.0:\n",
      "\u001b[2K      Successfully uninstalled mpmath-1.3.0\n",
      "\u001b[2K  Attempting uninstall: flatbuffersâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 1/91\u001b[0m [mpmath]\n",
      "\u001b[2K    Found existing installation: flatbuffers 25.9.232m 1/91\u001b[0m [mpmath]\n",
      "\u001b[2K    Uninstalling flatbuffers-25.9.23:â”â”â”â”â”â”â”\u001b[0m \u001b[32m 1/91\u001b[0m [mpmath]\n",
      "\u001b[2K      Successfully uninstalled flatbuffers-25.9.23[32m 1/91\u001b[0m [mpmath]\n",
      "\u001b[2K  Attempting uninstall: durationpyâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 1/91\u001b[0m [mpmath]\n",
      "\u001b[2K    Found existing installation: durationpy 0.10 \u001b[32m 1/91\u001b[0m [mpmath]\n",
      "\u001b[2K    Uninstalling durationpy-0.10:â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 1/91\u001b[0m [mpmath]\n",
      "\u001b[2K      Successfully uninstalled durationpy-0.100m \u001b[32m 1/91\u001b[0m [mpmath]\n",
      "\u001b[2K  Attempting uninstall: zippâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 1/91\u001b[0m [mpmath]\n",
      "\u001b[2K    Found existing installation: zipp 3.23.0\u001b[0m \u001b[32m 1/91\u001b[0m [mpmath]\n",
      "\u001b[2K    Uninstalling zipp-3.23.0:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 1/91\u001b[0m [mpmath]\n",
      "\u001b[2K      Successfully uninstalled zipp-3.23.0â”â”\u001b[0m \u001b[32m 1/91\u001b[0m [mpmath]\n",
      "\u001b[2K  Attempting uninstall: websocketsâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 1/91\u001b[0m [mpmath]\n",
      "\u001b[2K    Found existing installation: websockets 15.0.1[32m 1/91\u001b[0m [mpmath]\n",
      "\u001b[2K    Uninstalling websockets-15.0.1:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/91\u001b[0m [websockets]\n",
      "\u001b[2K      Successfully uninstalled websockets-15.0.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/91\u001b[0m [websockets]\n",
      "\u001b[2K  Attempting uninstall: websocket-clientâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/91\u001b[0m [websockets]\n",
      "\u001b[2K    Found existing installation: websocket-client 1.9.0â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/91\u001b[0m [websockets]\n",
      "\u001b[2K    Uninstalling websocket-client-1.9.0:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/91\u001b[0m [websockets]\n",
      "\u001b[2K      Successfully uninstalled websocket-client-1.9.0â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/91\u001b[0m [websockets]\n",
      "\u001b[2K  Attempting uninstall: uvloopâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/91\u001b[0m [websockets]\n",
      "\u001b[2K    Found existing installation: uvloop 0.22.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/91\u001b[0m [websockets]\n",
      "\u001b[2K    Uninstalling uvloop-0.22.1:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/91\u001b[0m [uvloop]\n",
      "\u001b[2K      Successfully uninstalled uvloop-0.22.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/91\u001b[0m [uvloop]\n",
      "\u001b[2K  Attempting uninstall: urllib3â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/91\u001b[0m [uvloop]\n",
      "\u001b[2K    Found existing installation: urllib3 2.3.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/91\u001b[0m [uvloop]\n",
      "\u001b[2K    Uninstalling urllib3-2.3.0:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/91\u001b[0m [uvloop]\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.3.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/91\u001b[0m [uvloop]\n",
      "\u001b[2K  Attempting uninstall: typing-extensionsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 9/91\u001b[0m [urllib3]\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.15.0â”â”â”â”â”\u001b[0m \u001b[32m 9/91\u001b[0m [urllib3]\n",
      "\u001b[2K    Uninstalling typing_extensions-4.15.0:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 9/91\u001b[0m [urllib3]\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.15.0â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/91\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: tqdmâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/91\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: tqdm 4.67.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/91\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling tqdm-4.67.1:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/91\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled tqdm-4.67.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/91\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: tomli0mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11/91\u001b[0m [tqdm]tensions]\n",
      "\u001b[2K    Found existing installation: tomli 2.3.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11/91\u001b[0m [tqdm]\n",
      "\u001b[2K    Uninstalling tomli-2.3.0:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11/91\u001b[0m [tqdm]\n",
      "\u001b[2K      Successfully uninstalled tomli-2.3.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11/91\u001b[0m [tqdm]\n",
      "\u001b[2K  Attempting uninstall: tenacityâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11/91\u001b[0m [tqdm]\n",
      "\u001b[2K    Found existing installation: tenacity 9.1.2â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11/91\u001b[0m [tqdm]\n",
      "\u001b[2K    Uninstalling tenacity-9.1.2:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11/91\u001b[0m [tqdm]\n",
      "\u001b[2K      Successfully uninstalled tenacity-9.1.2â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11/91\u001b[0m [tqdm]\n",
      "\u001b[2K  Attempting uninstall: sympy90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13/91\u001b[0m [tenacity]\n",
      "\u001b[2K    Found existing installation: sympy 1.14.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13/91\u001b[0m [tenacity]\n",
      "\u001b[2K    Uninstalling sympy-1.14.0:90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14/91\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.14.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14/91\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: shellinghamâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14/91\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: shellingham 1.5.4â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14/91\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling shellingham-1.5.4:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14/91\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled shellingham-1.5.4â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14/91\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: PyYAMLâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14/91\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: PyYAML 6.0.3â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14/91\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling PyYAML-6.0.3:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14/91\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled PyYAML-6.0.3â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14/91\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: python-dotenvâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17/91\u001b[0m [PyYAML]\n",
      "\u001b[2K    Found existing installation: python-dotenv 1.2.1â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17/91\u001b[0m [PyYAML]\n",
      "\u001b[2K    Uninstalling python-dotenv-1.2.1:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17/91\u001b[0m [PyYAML]\n",
      "\u001b[2K      Successfully uninstalled python-dotenv-1.2.1â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17/91\u001b[0m [PyYAML]\n",
      "\u001b[2K  Attempting uninstall: pyproject_hooksâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17/91\u001b[0m [PyYAML]\n",
      "\u001b[2K    Found existing installation: pyproject_hooks 1.2.0â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17/91\u001b[0m [PyYAML]\n",
      "\u001b[2K    Uninstalling pyproject_hooks-1.2.0:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17/91\u001b[0m [PyYAML]\n",
      "\u001b[2K      Successfully uninstalled pyproject_hooks-1.2.0â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17/91\u001b[0m [PyYAML]\n",
      "\u001b[2K  Attempting uninstall: pygments90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19/91\u001b[0m [pyproject_hooks]\n",
      "\u001b[2K    Found existing installation: Pygments 2.19.2â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19/91\u001b[0m [pyproject_hooks]\n",
      "\u001b[2K    Uninstalling Pygments-2.19.2:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19/91\u001b[0m [pyproject_hooks]\n",
      "\u001b[2K      Successfully uninstalled Pygments-2.19.2â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19/91\u001b[0m [pyproject_hooks]\n",
      "\u001b[2K  Attempting uninstall: pyasn1\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20/91\u001b[0m [pygments]ks]\n",
      "\u001b[2K    Found existing installation: pyasn1 0.6.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20/91\u001b[0m [pygments]\n",
      "\u001b[2K    Uninstalling pyasn1-0.6.1:mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20/91\u001b[0m [pygments]\n",
      "\u001b[2K      Successfully uninstalled pyasn1-0.6.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20/91\u001b[0m [pygments]\n",
      "\u001b[2K  Attempting uninstall: protobuf[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21/91\u001b[0m [pyasn1]\n",
      "\u001b[2K    Found existing installation: protobuf 6.33.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21/91\u001b[0m [pyasn1]\n",
      "\u001b[2K    Uninstalling protobuf-6.33.1:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21/91\u001b[0m [pyasn1]\n",
      "\u001b[2K      Successfully uninstalled protobuf-6.33.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21/91\u001b[0m [pyasn1]\n",
      "\u001b[2K  Attempting uninstall: packaging90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m22/91\u001b[0m [protobuf]\n",
      "\u001b[2K    Found existing installation: packaging 25.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m22/91\u001b[0m [protobuf]\n",
      "\u001b[2K    Uninstalling packaging-25.0:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m22/91\u001b[0m [protobuf]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m22/91\u001b[0m [protobuf]\n",
      "\u001b[2K  Attempting uninstall: overrides[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23/91\u001b[0m [packaging]\n",
      "\u001b[2K    Found existing installation: overrides 7.7.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23/91\u001b[0m [packaging]\n",
      "\u001b[2K    Uninstalling overrides-7.7.0:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23/91\u001b[0m [packaging]\n",
      "\u001b[2K      Successfully uninstalled overrides-7.7.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23/91\u001b[0m [packaging]\n",
      "\u001b[2K  Attempting uninstall: oauthlibmâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23/91\u001b[0m [packaging]\n",
      "\u001b[2K    Found existing installation: oauthlib 3.3.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23/91\u001b[0m [packaging]\n",
      "\u001b[2K    Uninstalling oauthlib-3.3.1:mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23/91\u001b[0m [packaging]\n",
      "\u001b[2K      Successfully uninstalled oauthlib-3.3.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23/91\u001b[0m [packaging]\n",
      "\u001b[2K  Attempting uninstall: numpy\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26/91\u001b[0m [oauthlib]\n",
      "\u001b[2K    Found existing installation: numpy 2.0.2â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26/91\u001b[0m [oauthlib]\n",
      "\u001b[2K    Uninstalling numpy-2.0.2:\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27/91\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.0.2â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27/91\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: mmh3â•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27/91\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: mmh3 5.2.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27/91\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling mmh3-5.2.0:m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27/91\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled mmh3-5.2.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m28/91\u001b[0m [mmh3]\n",
      "\u001b[2K  Attempting uninstall: mdurlm\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m28/91\u001b[0m [mmh3]\n",
      "\u001b[2K    Found existing installation: mdurl 0.1.2â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m28/91\u001b[0m [mmh3]\n",
      "\u001b[2K    Uninstalling mdurl-0.1.2:m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m28/91\u001b[0m [mmh3]\n",
      "\u001b[2K      Successfully uninstalled mdurl-0.1.2â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m28/91\u001b[0m [mmh3]\n",
      "\u001b[2K  Attempting uninstall: idna0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m28/91\u001b[0m [mmh3]\n",
      "\u001b[2K    Found existing installation: idna 3.11â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m28/91\u001b[0m [mmh3]\n",
      "\u001b[2K    Uninstalling idna-3.11:[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m28/91\u001b[0m [mmh3]\n",
      "\u001b[2K      Successfully uninstalled idna-3.11â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m28/91\u001b[0m [mmh3]\n",
      "\u001b[2K  Attempting uninstall: humanfriendly90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30/91\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: humanfriendly 10.0â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30/91\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling humanfriendly-10.0:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30/91\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled humanfriendly-10.0â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30/91\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: httptools90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30/91\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: httptools 0.7.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30/91\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling httptools-0.7.1:90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30/91\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled httptools-0.7.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30/91\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: hf-xetm\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30/91\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: hf-xet 1.2.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30/91\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling hf-xet-1.2.0:m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30/91\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled hf-xet-1.2.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30/91\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: h11[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m33/91\u001b[0m [hf-xet]\n",
      "\u001b[2K    Found existing installation: h11 0.16.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m33/91\u001b[0m [hf-xet]\n",
      "\u001b[2K    Uninstalling h11-0.16.0:\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m33/91\u001b[0m [hf-xet]\n",
      "\u001b[2K      Successfully uninstalled h11-0.16.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m33/91\u001b[0m [hf-xet]\n",
      "\u001b[2K  Attempting uninstall: fsspec0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m33/91\u001b[0m [hf-xet]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.10.0â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m33/91\u001b[0m [hf-xet]\n",
      "\u001b[2K    Uninstalling fsspec-2025.10.0:90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m33/91\u001b[0m [hf-xet]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.10.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m33/91\u001b[0m [hf-xet]\n",
      "\u001b[2K  Attempting uninstall: filelockâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35/91\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: filelock 3.19.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35/91\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling filelock-3.19.1:\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35/91\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled filelock-3.19.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35/91\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: distro[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35/91\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: distro 1.9.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35/91\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling distro-1.9.0:[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35/91\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled distro-1.9.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35/91\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: click[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m37/91\u001b[0m [distro]\n",
      "\u001b[2K    Found existing installation: click 8.1.8â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m37/91\u001b[0m [distro]\n",
      "\u001b[2K    Uninstalling click-8.1.8:â•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m37/91\u001b[0m [distro]\n",
      "\u001b[2K      Successfully uninstalled click-8.1.8â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m37/91\u001b[0m [distro]\n",
      "\u001b[2K  Attempting uninstall: charset_normalizerâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m37/91\u001b[0m [distro]\n",
      "\u001b[2K    Found existing installation: charset-normalizer 3.4.4â”â”â”â”â”\u001b[0m \u001b[32m37/91\u001b[0m [distro]\n",
      "\u001b[2K    Uninstalling charset-normalizer-3.4.4:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m37/91\u001b[0m [distro]\n",
      "\u001b[2K      Successfully uninstalled charset-normalizer-3.4.4â”â”â”â”â”â”â”\u001b[0m \u001b[32m37/91\u001b[0m [distro]\n",
      "\u001b[2K  Attempting uninstall: certifi90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39/91\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Found existing installation: certifi 2025.11.12â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39/91\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Uninstalling certifi-2025.11.12:[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39/91\u001b[0m [charset_normalizer]\n",
      "\u001b[2K      Successfully uninstalled certifi-2025.11.12â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39/91\u001b[0m [charset_normalizer]\n",
      "\u001b[2K  Attempting uninstall: cachetoolsm\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39/91\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Found existing installation: cachetools 6.2.2â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39/91\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Uninstalling cachetools-6.2.2:m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39/91\u001b[0m [charset_normalizer]\n",
      "\u001b[2K      Successfully uninstalled cachetools-6.2.2â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39/91\u001b[0m [charset_normalizer]\n",
      "\u001b[2K  Attempting uninstall: bcryptâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39/91\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Found existing installation: bcrypt 5.0.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39/91\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Uninstalling bcrypt-5.0.0:â•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39/91\u001b[0m [charset_normalizer]\n",
      "\u001b[2K      Successfully uninstalled bcrypt-5.0.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39/91\u001b[0m [charset_normalizer]\n",
      "\u001b[2K  Attempting uninstall: backoff[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42/91\u001b[0m [bcrypt]malizer]\n",
      "\u001b[2K    Found existing installation: backoff 2.2.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42/91\u001b[0m [bcrypt]\n",
      "\u001b[2K    Uninstalling backoff-2.2.1:â•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42/91\u001b[0m [bcrypt]\n",
      "\u001b[2K      Successfully uninstalled backoff-2.2.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42/91\u001b[0m [bcrypt]\n",
      "\u001b[2K  Attempting uninstall: annotated-types0mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42/91\u001b[0m [bcrypt]\n",
      "\u001b[2K    Found existing installation: annotated-types 0.7.0â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42/91\u001b[0m [bcrypt]\n",
      "\u001b[2K    Uninstalling annotated-types-0.7.0:0mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42/91\u001b[0m [bcrypt]\n",
      "\u001b[2K      Successfully uninstalled annotated-types-0.7.0â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42/91\u001b[0m [bcrypt]\n",
      "\u001b[2K  Attempting uninstall: uvicornâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42/91\u001b[0m [bcrypt]\n",
      "\u001b[2K    Found existing installation: uvicorn 0.38.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42/91\u001b[0m [bcrypt]\n",
      "\u001b[2K    Uninstalling uvicorn-0.38.0:\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42/91\u001b[0m [bcrypt]\n",
      "\u001b[2K      Successfully uninstalled uvicorn-0.38.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42/91\u001b[0m [bcrypt]\n",
      "\u001b[2K  Attempting uninstall: typing-inspectionm\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46/91\u001b[0m [uvicorn]\n",
      "\u001b[2K    Found existing installation: typing-inspection 0.4.2â”â”â”â”â”â”\u001b[0m \u001b[32m46/91\u001b[0m [uvicorn]\n",
      "\u001b[2K    Uninstalling typing-inspection-0.4.2:0mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46/91\u001b[0m [uvicorn]\n",
      "\u001b[2K      Successfully uninstalled typing-inspection-0.4.2â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46/91\u001b[0m [uvicorn]\n",
      "\u001b[2K  Attempting uninstall: rsaâ”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48/91\u001b[0m [typer-slim]\n",
      "\u001b[2K    Found existing installation: rsa 4.9.10mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48/91\u001b[0m [typer-slim]\n",
      "\u001b[2K    Uninstalling rsa-4.9.1:0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48/91\u001b[0m [typer-slim]\n",
      "\u001b[2K      Successfully uninstalled rsa-4.9.1[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48/91\u001b[0m [typer-slim]\n",
      "\u001b[2K  Attempting uninstall: requests0mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48/91\u001b[0m [typer-slim]\n",
      "\u001b[2K    Found existing installation: requests 2.32.5â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48/91\u001b[0m [typer-slim]\n",
      "\u001b[2K    Uninstalling requests-2.32.5:mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48/91\u001b[0m [typer-slim]\n",
      "\u001b[2K      Successfully uninstalled requests-2.32.5â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48/91\u001b[0m [typer-slim]\n",
      "\u001b[2K  Attempting uninstall: python-dateutil\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50/91\u001b[0m [requests]\n",
      "\u001b[2K    Found existing installation: python-dateutil 2.9.0.post0â”â”\u001b[0m \u001b[32m50/91\u001b[0m [requests]\n",
      "\u001b[2K    Uninstalling python-dateutil-2.9.0.post0:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50/91\u001b[0m [requests]\n",
      "\u001b[2K      Successfully uninstalled python-dateutil-2.9.0.post0â”â”â”â”\u001b[0m \u001b[32m50/91\u001b[0m [requests]\n",
      "\u001b[2K  Attempting uninstall: pydantic-core0mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51/91\u001b[0m [python-dateutil]\n",
      "\u001b[2K    Found existing installation: pydantic_core 2.41.5â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51/91\u001b[0m [python-dateutil]\n",
      "\u001b[2K    Uninstalling pydantic_core-2.41.5:0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51/91\u001b[0m [python-dateutil]\n",
      "\u001b[2K      Successfully uninstalled pydantic_core-2.41.5â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51/91\u001b[0m [python-dateutil]\n",
      "\u001b[2K  Attempting uninstall: pyasn1-modulesmâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52/91\u001b[0m [pydantic-core]\n",
      "\u001b[2K    Found existing installation: pyasn1_modules 0.4.2â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52/91\u001b[0m [pydantic-core]\n",
      "\u001b[2K    Uninstalling pyasn1_modules-0.4.2:0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52/91\u001b[0m [pydantic-core]\n",
      "\u001b[2K      Successfully uninstalled pyasn1_modules-0.4.2â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52/91\u001b[0m [pydantic-core]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-proto0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54/91\u001b[0m [pulsar-client]]\n",
      "\u001b[2K    Found existing installation: opentelemetry-proto 1.39.0â”â”â”\u001b[0m \u001b[32m54/91\u001b[0m [pulsar-client]\n",
      "\u001b[2K    Uninstalling opentelemetry-proto-1.39.0:0mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54/91\u001b[0m [pulsar-client]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-proto-1.39.0â”â”â”â”â”\u001b[0m \u001b[32m54/91\u001b[0m [pulsar-client]\n",
      "\u001b[2K  Attempting uninstall: markdown-it-py90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55/91\u001b[0m [opentelemetry-proto]\n",
      "\u001b[2K    Found existing installation: markdown-it-py 3.0.0â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55/91\u001b[0m [opentelemetry-proto]\n",
      "\u001b[2K    Uninstalling markdown-it-py-3.0.0:\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55/91\u001b[0m [opentelemetry-proto]\n",
      "\u001b[2K      Successfully uninstalled markdown-it-py-3.0.0â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55/91\u001b[0m [opentelemetry-proto]\n",
      "\u001b[2K  Attempting uninstall: importlib-resources[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56/91\u001b[0m [markdown-it-py]]\n",
      "\u001b[2K    Found existing installation: importlib_resources 6.5.2â”â”â”â”\u001b[0m \u001b[32m56/91\u001b[0m [markdown-it-py]\n",
      "\u001b[2K    Uninstalling importlib_resources-6.5.2:[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56/91\u001b[0m [markdown-it-py]\n",
      "\u001b[2K      Successfully uninstalled importlib_resources-6.5.2â”â”â”â”â”â”\u001b[0m \u001b[32m56/91\u001b[0m [markdown-it-py]\n",
      "\u001b[2K  Attempting uninstall: importlib-metadata\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56/91\u001b[0m [markdown-it-py]\n",
      "\u001b[2K    Found existing installation: importlib_metadata 8.7.0â”â”â”â”â”\u001b[0m \u001b[32m56/91\u001b[0m [markdown-it-py]\n",
      "\u001b[2K    Uninstalling importlib_metadata-8.7.0:\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56/91\u001b[0m [markdown-it-py]\n",
      "\u001b[2K      Successfully uninstalled importlib_metadata-8.7.0â”â”â”â”â”â”â”\u001b[0m \u001b[32m56/91\u001b[0m [markdown-it-py]\n",
      "\u001b[2K  Attempting uninstall: httpcore\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56/91\u001b[0m [markdown-it-py]\n",
      "\u001b[2K    Found existing installation: httpcore 1.0.9â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56/91\u001b[0m [markdown-it-py]\n",
      "\u001b[2K    Uninstalling httpcore-1.0.9:\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56/91\u001b[0m [markdown-it-py]\n",
      "\u001b[2K      Successfully uninstalled httpcore-1.0.90m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59/91\u001b[0m [httpcore]y]\n",
      "\u001b[2K  Attempting uninstall: grpcio[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59/91\u001b[0m [httpcore]\n",
      "\u001b[2K    Found existing installation: grpcio 1.76.00mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59/91\u001b[0m [httpcore]\n",
      "\u001b[2K    Uninstalling grpcio-1.76.0:0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59/91\u001b[0m [httpcore]\n",
      "\u001b[2K      Successfully uninstalled grpcio-1.76.0[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59/91\u001b[0m [httpcore]\n",
      "\u001b[2K  Attempting uninstall: googleapis-common-protos\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60/91\u001b[0m [grpcio]\n",
      "\u001b[2K    Found existing installation: googleapis-common-protos 1.72.00m \u001b[32m60/91\u001b[0m [grpcio]\n",
      "\u001b[2K    Uninstalling googleapis-common-protos-1.72.0:â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60/91\u001b[0m [grpcio]\n",
      "\u001b[2K      Successfully uninstalled googleapis-common-protos-1.72.0â”â”â”â”\u001b[0m \u001b[32m61/91\u001b[0m [googleapis-common-protos]\n",
      "\u001b[2K  Attempting uninstall: exceptiongroup\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61/91\u001b[0m [googleapis-common-protos]\n",
      "\u001b[2K    Found existing installation: exceptiongroup 1.3.0â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61/91\u001b[0m [googleapis-common-protos]\n",
      "\u001b[2K    Uninstalling exceptiongroup-1.3.0:mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61/91\u001b[0m [googleapis-common-protos]\n",
      "\u001b[2K      Successfully uninstalled exceptiongroup-1.3.0â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61/91\u001b[0m [googleapis-common-protos]\n",
      "\u001b[2K  Attempting uninstall: coloredlogs[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61/91\u001b[0m [googleapis-common-protos]\n",
      "\u001b[2K    Found existing installation: coloredlogs 15.0.1â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61/91\u001b[0m [googleapis-common-protos]\n",
      "\u001b[2K    Uninstalling coloredlogs-15.0.1:91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61/91\u001b[0m [googleapis-common-protos]\n",
      "\u001b[2K      Successfully uninstalled coloredlogs-15.0.1\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63/91\u001b[0m [coloredlogs]n-protos]\n",
      "\u001b[2K  Attempting uninstall: richâ”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63/91\u001b[0m [coloredlogs]\n",
      "\u001b[2K    Found existing installation: rich 14.2.0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63/91\u001b[0m [coloredlogs]\n",
      "\u001b[2K    Uninstalling rich-14.2.0:â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63/91\u001b[0m [coloredlogs]\n",
      "\u001b[2K      Successfully uninstalled rich-14.2.0[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63/91\u001b[0m [coloredlogs]\n",
      "\u001b[2K  Attempting uninstall: requests-oauthlib\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66/91\u001b[0m [rich]gs]\n",
      "\u001b[2K    Found existing installation: requests-oauthlib 2.0.0â”â”â”â”â”â”\u001b[0m \u001b[32m66/91\u001b[0m [rich]\n",
      "\u001b[2K    Uninstalling requests-oauthlib-2.0.0:mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66/91\u001b[0m [rich]\n",
      "\u001b[2K      Successfully uninstalled requests-oauthlib-2.0.0â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66/91\u001b[0m [rich]\n",
      "\u001b[2K  Attempting uninstall: pydanticâ”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66/91\u001b[0m [rich]\n",
      "\u001b[2K    Found existing installation: pydantic 2.12.4[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66/91\u001b[0m [rich]\n",
      "\u001b[2K    Uninstalling pydantic-2.12.4:\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66/91\u001b[0m [rich]\n",
      "\u001b[2K      Successfully uninstalled pydantic-2.12.4m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66/91\u001b[0m [rich]\n",
      "\u001b[2K  Attempting uninstall: posthogâ”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68/91\u001b[0m [pydantic]\n",
      "\u001b[2K    Found existing installation: posthog 5.4.0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68/91\u001b[0m [pydantic]\n",
      "\u001b[2K    Uninstalling posthog-5.4.0:â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68/91\u001b[0m [pydantic]\n",
      "\u001b[2K      Successfully uninstalled posthog-5.4.0[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68/91\u001b[0m [pydantic]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-exporter-otlp-proto-commonâ”â”\u001b[0m \u001b[32m69/91\u001b[0m [posthog]\n",
      "\u001b[2K    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.39.0m [posthog]\n",
      "\u001b[2K    Uninstalling opentelemetry-exporter-otlp-proto-common-1.39.0:m \u001b[32m69/91\u001b[0m [posthog]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.39.0[0m [posthog]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-api0mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69/91\u001b[0m [posthog]\n",
      "\u001b[2K    Found existing installation: opentelemetry-api 1.39.0â”â”â”â”â”\u001b[0m \u001b[32m69/91\u001b[0m [posthog]\n",
      "\u001b[2K    Uninstalling opentelemetry-api-1.39.0:mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69/91\u001b[0m [posthog]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-api-1.39.0â”â”â”â”â”â”â”\u001b[0m \u001b[32m69/91\u001b[0m [posthog]\n",
      "\u001b[2K  Attempting uninstall: onnxruntimeâ”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71/91\u001b[0m [opentelemetry-api]\n",
      "\u001b[2K    Found existing installation: onnxruntime 1.19.290mâ”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71/91\u001b[0m [opentelemetry-api]\n",
      "\u001b[2K    Uninstalling onnxruntime-1.19.2:[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71/91\u001b[0m [opentelemetry-api]\n",
      "\u001b[2K      Successfully uninstalled onnxruntime-1.19.2\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72/91\u001b[0m [onnxruntime]i]\n",
      "\u001b[2K  Attempting uninstall: google-authâ”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72/91\u001b[0m [onnxruntime]\n",
      "\u001b[2K    Found existing installation: google-auth 2.43.090mâ”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72/91\u001b[0m [onnxruntime]\n",
      "\u001b[2K    Uninstalling google-auth-2.43.0:[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72/91\u001b[0m [onnxruntime]\n",
      "\u001b[2K      Successfully uninstalled google-auth-2.43.0\u001b[90mâ”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72/91\u001b[0m [onnxruntime]\n",
      "\u001b[2K  Attempting uninstall: buildâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m73/91\u001b[0m [google-auth]\n",
      "\u001b[2K    Found existing installation: build 1.3.0mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m73/91\u001b[0m [google-auth]\n",
      "\u001b[2K    Uninstalling build-1.3.0:â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m73/91\u001b[0m [google-auth]\n",
      "\u001b[2K      Successfully uninstalled build-1.3.090mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m73/91\u001b[0m [google-auth]\n",
      "\u001b[2K  Attempting uninstall: anyioâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m74/91\u001b[0m [build]h]\n",
      "\u001b[2K    Found existing installation: anyio 4.11.0â•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m74/91\u001b[0m [build]\n",
      "\u001b[2K    Uninstalling anyio-4.11.0:â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m74/91\u001b[0m [build]\n",
      "\u001b[2K      Successfully uninstalled anyio-4.11.01mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m74/91\u001b[0m [build]\n",
      "\u001b[2K  Attempting uninstall: watchfilesâ”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m75/91\u001b[0m [anyio]\n",
      "\u001b[2K    Found existing installation: watchfiles 1.1.1m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m75/91\u001b[0m [anyio]\n",
      "\u001b[2K    Uninstalling watchfiles-1.1.1:â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m75/91\u001b[0m [anyio]\n",
      "\u001b[2K      Successfully uninstalled watchfiles-1.1.1[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m75/91\u001b[0m [anyio]\n",
      "\u001b[2K  Attempting uninstall: typerâ”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m75/91\u001b[0m [anyio]\n",
      "\u001b[2K    Found existing installation: typer 0.20.0â•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m75/91\u001b[0m [anyio]\n",
      "\u001b[2K    Uninstalling typer-0.20.0:â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m75/91\u001b[0m [anyio]\n",
      "\u001b[2K      Successfully uninstalled typer-0.20.01mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m75/91\u001b[0m [anyio]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-semantic-conventions0mâ”â”â”â”â”â”\u001b[0m \u001b[32m77/91\u001b[0m [typer]\n",
      "\u001b[2K    Found existing installation: opentelemetry-semantic-conventions 0.60b0/91\u001b[0m [typer]\n",
      "\u001b[2K    Uninstalling opentelemetry-semantic-conventions-0.60b0:â”\u001b[0m \u001b[32m79/91\u001b[0m [opentelemetry-semantic-conventions]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-semantic-conventions-0.60b0[0m [opentelemetry-semantic-conventions]\n",
      "\u001b[2K  Attempting uninstall: kubernetesâ”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m \u001b[32m79/91\u001b[0m [opentelemetry-semantic-conventions]\n",
      "\u001b[2K    Found existing installation: kubernetes 34.1.00mâ”â”â”â”\u001b[0m \u001b[32m79/91\u001b[0m [opentelemetry-semantic-conventions]\n",
      "\u001b[2K    Uninstalling kubernetes-34.1.0:â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m \u001b[32m80/91\u001b[0m [kubernetes]c-conventions]\n",
      "\u001b[2K      Successfully uninstalled kubernetes-34.1.0â•º\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m \u001b[32m80/91\u001b[0m [kubernetes]\n",
      "\u001b[2K  Attempting uninstall: httpxâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m \u001b[32m80/91\u001b[0m [kubernetes]\n",
      "\u001b[2K    Found existing installation: httpx 0.28.190mâ•º\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m \u001b[32m80/91\u001b[0m [kubernetes]\n",
      "\u001b[2K    Uninstalling httpx-0.28.1:â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m \u001b[32m80/91\u001b[0m [kubernetes]\n",
      "\u001b[2K      Successfully uninstalled httpx-0.28.1\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m \u001b[32m80/91\u001b[0m [kubernetes]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-sdkâ”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m \u001b[32m81/91\u001b[0m [httpx]]\n",
      "\u001b[2K    Found existing installation: opentelemetry-sdk 1.39.0mâ”â”â”â”\u001b[0m \u001b[32m81/91\u001b[0m [httpx]\n",
      "\u001b[2K    Uninstalling opentelemetry-sdk-1.39.0:m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m \u001b[32m81/91\u001b[0m [httpx]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-sdk-1.39.090mâ”â”â”â”\u001b[0m \u001b[32m81/91\u001b[0m [httpx]\n",
      "\u001b[2K  Attempting uninstall: huggingface-hubâ”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”\u001b[0m \u001b[32m82/91\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.36.0[90mâ”â”â”\u001b[0m \u001b[32m82/91\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.36.0:\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”\u001b[0m \u001b[32m82/91\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.36.0m\u001b[90mâ”â”â”\u001b[0m \u001b[32m82/91\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K  Attempting uninstall: tokenizersâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”\u001b[0m \u001b[32m85/91\u001b[0m [fastapi]ace-hub]\n",
      "\u001b[2K    Found existing installation: tokenizers 0.22.1â•º\u001b[0m\u001b[90mâ”â”\u001b[0m \u001b[32m85/91\u001b[0m [fastapi]\n",
      "\u001b[2K    Uninstalling tokenizers-0.22.1:â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”\u001b[0m \u001b[32m85/91\u001b[0m [fastapi]\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.22.10mâ•º\u001b[0m\u001b[90mâ”â”\u001b[0m \u001b[32m85/91\u001b[0m [fastapi]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-exporter-otlp-proto-grpc\u001b[0m \u001b[32m85/91\u001b[0m [fastapi]\n",
      "\u001b[2K    Found existing installation: opentelemetry-exporter-otlp-proto-grpc 1.39.0[0m [fastapi]\n",
      "\u001b[2K    Uninstalling opentelemetry-exporter-otlp-proto-grpc-1.39.0:[0m \u001b[32m85/91\u001b[0m [fastapi]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-exporter-otlp-proto-grpc-1.39.01\u001b[0m [fastapi]\n",
      "\u001b[2K  Attempting uninstall: chromadbâ”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m \u001b[32m88/91\u001b[0m [opentelemetry-exporter-otlp-proto-grpc]\n",
      "\u001b[2K    Found existing installation: chromadb 1.3.5 \u001b[32m88/91\u001b[0m [opentelemetry-exporter-otlp-proto-grpc]\n",
      "\u001b[2K    Uninstalling chromadb-1.3.5:â”\u001b[0m\u001b[90mâ•º\u001b[0m \u001b[32m88/91\u001b[0m [opentelemetry-exporter-otlp-proto-grpc]\n",
      "\u001b[2K      Successfully uninstalled chromadb-1.3.50m \u001b[32m88/91\u001b[0m [opentelemetry-exporter-otlp-proto-grpc]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91/91\u001b[0m [chromadb]chromadb]otlp-proto-grpc]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastembed 0.7.3 requires huggingface-hub<1.0,>=0.20, but you have huggingface-hub 1.1.7 which is incompatible.\n",
      "transformers 4.57.1 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.1.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-6.0.3 annotated-doc-0.0.4 annotated-types-0.7.0 anyio-4.12.0 asgiref-3.11.0 backoff-2.2.1 bcrypt-5.0.0 build-1.3.0 cachetools-6.2.2 certifi-2025.11.12 charset_normalizer-3.4.4 chroma-hnswlib-0.7.3 chromadb-0.4.22 click-8.1.8 coloredlogs-15.0.1 distro-1.9.0 durationpy-0.10 exceptiongroup-1.3.1 fastapi-0.123.5 filelock-3.19.1 flatbuffers-25.9.23 fsspec-2025.10.0 google-auth-2.43.0 googleapis-common-protos-1.72.0 grpcio-1.76.0 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httptools-0.7.1 httpx-0.28.1 huggingface-hub-1.1.7 humanfriendly-10.0 idna-3.11 importlib-metadata-8.7.0 importlib-resources-6.5.2 kubernetes-34.1.0 markdown-it-py-3.0.0 mdurl-0.1.2 mmh3-5.2.0 mpmath-1.3.0 numpy-2.0.2 oauthlib-3.3.1 onnxruntime-1.19.2 opentelemetry-api-1.39.0 opentelemetry-exporter-otlp-proto-common-1.39.0 opentelemetry-exporter-otlp-proto-grpc-1.39.0 opentelemetry-instrumentation-0.60b0 opentelemetry-instrumentation-asgi-0.60b0 opentelemetry-instrumentation-fastapi-0.60b0 opentelemetry-proto-1.39.0 opentelemetry-sdk-1.39.0 opentelemetry-semantic-conventions-0.60b0 opentelemetry-util-http-0.60b0 overrides-7.7.0 packaging-25.0 posthog-6.9.3 protobuf-6.33.1 pulsar-client-3.8.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.12.5 pydantic-core-2.41.5 pygments-2.19.2 pypika-0.48.9 pyproject_hooks-1.2.0 python-dateutil-2.9.0.post0 python-dotenv-1.2.1 requests-2.32.5 requests-oauthlib-2.0.0 rich-14.2.0 rsa-4.9.1 shellingham-1.5.4 six-1.17.0 starlette-0.49.3 sympy-1.14.0 tenacity-9.1.2 tokenizers-0.22.1 tomli-2.3.0 tqdm-4.67.1 typer-0.20.0 typer-slim-0.20.0 typing-extensions-4.15.0 typing-inspection-0.4.2 urllib3-2.3.0 uvicorn-0.38.0 uvloop-0.22.1 watchfiles-1.1.1 websocket-client-1.9.0 websockets-15.0.1 wrapt-1.17.3 zipp-3.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade chromadb==0.4.22 --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7704ce3",
   "metadata": {},
   "source": [
    "FAISS is battle-tested, reliable, and won't have database corruption issues. Let's implement this step-by-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ede8f107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Run this cell in your notebook\n",
    "!pip install faiss-cpu langchain -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44968c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ RELIABLE FAISS - Processing 18717 chunks\n",
      "Loading fast embedding model...\n",
      "Creating embeddings for 18717 chunks...\n",
      "âœ“ Created 18717 embeddings\n",
      "Embedding shape: (18717, 384)\n",
      "\n",
      "ğŸ”§ Creating FAISS index...\n",
      "âœ“ FAISS index created with 18717 vectors\n",
      "\n",
      "ğŸ’¾ Saving to disk...\n",
      "âœ“ FAISS index saved: ../../04_models/vector_index/faiss_index.bin\n",
      "âœ“ Texts saved: ../../04_models/vector_index/texts.pkl (18717 texts)\n",
      "âœ“ Metadata saved: ../../04_models/vector_index/metadata.pkl (18717 entries)\n",
      "âœ“ Embeddings saved: ../../04_models/vector_index/embeddings.npy\n",
      "âœ“ Index info saved: ../../04_models/vector_index/index_info.json\n",
      "\n",
      "==================================================\n",
      "âœ… FAISS INDEX CREATION COMPLETE\n",
      "==================================================\n",
      "\n",
      "ğŸ“Š STATS:\n",
      "Total chunks: 18717\n",
      "Embedding dimension: 384\n",
      "FAISS index size: 18717 vectors\n",
      "\n",
      "ğŸ§ª TEST QUERY:\n",
      "Query: 'automotive technology'\n",
      "Found 3 results:\n",
      "\n",
      "  Result 1 (distance: 0.6645):\n",
      "    Type: research_paper\n",
      "    Preview: (V2X), Internet of Things (IOT), public clouds, data analytics, artificial intelligence, digitalizat...\n",
      "\n",
      "  Result 2 (distance: 0.7099):\n",
      "    Type: research_paper\n",
      "    Preview: RESEARCH PAPER #13:\n",
      "  Title: Emerging Trends in the Automotive Industry Driven by Sustainable Techno...\n",
      "\n",
      "  Result 3 (distance: 0.7124):\n",
      "    Type: research_paper\n",
      "    Preview: RESEARCH PAPER #919:\n",
      "  Title: Mapping the Landscape of Romanian Automotive Research: A Bibliometric ...\n",
      "\n",
      "ğŸ“ Directory contents of ../../04_models/vector_index:\n",
      "  - chunks_metadata.pkl (11,772,358 bytes)\n",
      "  - metadata.pkl (431,988 bytes)\n",
      "  - faiss_index.bin (28,749,357 bytes)\n",
      "  - embeddings.npy (28,749,440 bytes)\n",
      "  - index_info.json (187 bytes)\n",
      "  - texts.pkl (11,096,569 bytes)\n",
      "\n",
      "==================================================\n",
      "ğŸ‰ RELIABLE VECTOR STORE READY!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== RELIABLE FAISS EMBEDDING SOLUTION =====\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from fastembed import TextEmbedding\n",
    "import faiss\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"ğŸš€ RELIABLE FAISS - Processing {len(all_chunks)} chunks\")\n",
    "\n",
    "# Extract texts\n",
    "texts = [chunk.page_content for chunk in all_chunks]\n",
    "metadatas = [chunk.metadata for chunk in all_chunks]\n",
    "\n",
    "# Setup paths\n",
    "vector_index_path = \"../../04_models/vector_index\"\n",
    "os.makedirs(vector_index_path, exist_ok=True)\n",
    "\n",
    "# 1. Create embeddings\n",
    "print(\"Loading fast embedding model...\")\n",
    "model = TextEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "print(f\"Creating embeddings for {len(texts)} chunks...\")\n",
    "embeddings = list(model.embed(texts))\n",
    "print(f\"âœ“ Created {len(embeddings)} embeddings\")\n",
    "\n",
    "# Convert to numpy array\n",
    "embeddings_array = np.array([emb.tolist() for emb in embeddings]).astype('float32')\n",
    "print(f\"Embedding shape: {embeddings_array.shape}\")\n",
    "\n",
    "# 2. Create FAISS index\n",
    "print(\"\\nğŸ”§ Creating FAISS index...\")\n",
    "dimension = embeddings_array.shape[1]\n",
    "\n",
    "# Create index (L2 distance - smaller is better)\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings_array)\n",
    "\n",
    "print(f\"âœ“ FAISS index created with {index.ntotal} vectors\")\n",
    "\n",
    "# 3. Save everything\n",
    "print(\"\\nğŸ’¾ Saving to disk...\")\n",
    "\n",
    "# Save FAISS index\n",
    "faiss_index_path = os.path.join(vector_index_path, \"faiss_index.bin\")\n",
    "faiss.write_index(index, faiss_index_path)\n",
    "print(f\"âœ“ FAISS index saved: {faiss_index_path}\")\n",
    "\n",
    "# Save texts\n",
    "texts_path = os.path.join(vector_index_path, \"texts.pkl\")\n",
    "with open(texts_path, \"wb\") as f:\n",
    "    pickle.dump(texts, f)\n",
    "print(f\"âœ“ Texts saved: {texts_path} ({len(texts)} texts)\")\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = os.path.join(vector_index_path, \"metadata.pkl\")\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(metadatas, f)\n",
    "print(f\"âœ“ Metadata saved: {metadata_path} ({len(metadatas)} entries)\")\n",
    "\n",
    "# Save embeddings for reference\n",
    "embeddings_path = os.path.join(vector_index_path, \"embeddings.npy\")\n",
    "np.save(embeddings_path, embeddings_array)\n",
    "print(f\"âœ“ Embeddings saved: {embeddings_path}\")\n",
    "\n",
    "# 4. Create index info\n",
    "index_info = {\n",
    "    \"total_chunks\": len(texts),\n",
    "    \"embedding_dim\": dimension,\n",
    "    \"created_at\": str(datetime.now()),\n",
    "    \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"index_type\": \"FAISS IndexFlatL2\"\n",
    "}\n",
    "\n",
    "info_path = os.path.join(vector_index_path, \"index_info.json\")\n",
    "import json\n",
    "with open(info_path, \"w\") as f:\n",
    "    json.dump(index_info, f, indent=2)\n",
    "print(f\"âœ“ Index info saved: {info_path}\")\n",
    "\n",
    "# 5. VERIFICATION\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"âœ… FAISS INDEX CREATION COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nğŸ“Š STATS:\")\n",
    "print(f\"Total chunks: {len(texts)}\")\n",
    "print(f\"Embedding dimension: {dimension}\")\n",
    "print(f\"FAISS index size: {index.ntotal} vectors\")\n",
    "\n",
    "# Test query\n",
    "print(\"\\nğŸ§ª TEST QUERY:\")\n",
    "test_query = \"automotive technology\"\n",
    "print(f\"Query: '{test_query}'\")\n",
    "\n",
    "# Create embedding for query\n",
    "query_embedding = np.array(list(model.embed([test_query]))[0].tolist()).astype('float32').reshape(1, -1)\n",
    "\n",
    "# Search\n",
    "k = 3\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "print(f\"Found {len(indices[0])} results:\")\n",
    "for i, (idx, distance) in enumerate(zip(indices[0], distances[0])):\n",
    "    if idx != -1:  # FAISS returns -1 if not enough results\n",
    "        preview = texts[idx][:100] + \"...\" if len(texts[idx]) > 100 else texts[idx]\n",
    "        doc_type = metadatas[idx].get('doc_type', 'unknown')\n",
    "        print(f\"\\n  Result {i+1} (distance: {distance:.4f}):\")\n",
    "        print(f\"    Type: {doc_type}\")\n",
    "        print(f\"    Preview: {preview}\")\n",
    "\n",
    "# 6. Directory listing\n",
    "print(f\"\\nğŸ“ Directory contents of {vector_index_path}:\")\n",
    "for file in os.listdir(vector_index_path):\n",
    "    file_path = os.path.join(vector_index_path, file)\n",
    "    size = os.path.getsize(file_path)\n",
    "    print(f\"  - {file} ({size:,} bytes)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ‰ RELIABLE VECTOR STORE READY!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a270b5",
   "metadata": {},
   "source": [
    "### Retriever prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9378ade",
   "metadata": {},
   "source": [
    "Load and test the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c79f7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Retriever imported!\n"
     ]
    }
   ],
   "source": [
    "def import_retriever():\n",
    "    current_dir = os.getcwd()\n",
    "    retriever_path = os.path.join(current_dir, 'rag_components', 'retriever.py')\n",
    "    spec = importlib.util.spec_from_file_location(\"retriever\", retriever_path)\n",
    "    retriever_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(retriever_module)\n",
    "    return retriever_module.DocumentAwareRetriever\n",
    "\n",
    "DocumentAwareRetriever = import_retriever()\n",
    "print(\"âœ… Retriever imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0365480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the retriever\n",
    "print(\"Initializing TF-IDF retriever...\")\n",
    "VECTOR_INDEX_PATH = \"../../04_models/vector_index\"\n",
    "retriever = DocumentAwareRetriever(VECTOR_INDEX_PATH)\n",
    "\n",
    "# Test the retriever with startups data\n",
    "print(\"\\nğŸ§ª Testing retriever with automotive-focused queries...\")\n",
    "\n",
    "test_queries = [\n",
    "    \"automotive startups\",\n",
    "    \"autonomous driving technology\", \n",
    "    \"generative AI in automotive\",\n",
    "    \"electric vehicle innovation\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    results = retriever.retrieve_with_sources(query, k=3)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nğŸ” Query: '{query}'\")\n",
    "        print(f\"ğŸ“Š Top result: {results[0]['source_file']}\")\n",
    "        print(f\"ğŸ“ Type: {results[0]['doc_type']}\")\n",
    "        print(f\"â­ Score: {results[0]['similarity_score']:.4f}\")\n",
    "        \n",
    "        # Check if startups data was retrieved\n",
    "        startups_found = any(doc['doc_type'] == 'startups_data' for doc in results)\n",
    "        print(f\"ğŸš€ Startups data included: {'âœ“' if startups_found else 'âœ—'}\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ No results for: '{query}'\")\n",
    "\n",
    "# Show document type distribution\n",
    "doc_type_counts = retriever.get_doc_type_counts()\n",
    "print(f\"\\nğŸ“ˆ Document type distribution:\")\n",
    "for doc_type, count in doc_type_counts.items():\n",
    "    print(f\"  - {doc_type}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb414153",
   "metadata": {},
   "source": [
    "Document type imbalance:\n",
    "\n",
    "Research papers: 12,466 chunks (72%)\n",
    "Patents: 4,198 chunks (24%)\n",
    "Startups: 1,366 chunks (4%) â† Only 4% of your data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34ea840",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09333fc3",
   "metadata": {},
   "source": [
    "Updated code using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca9c42bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Direct FAISS index test from Notebook 2\n",
      "============================================================\n",
      "Index path: ../../04_models/vector_index\n",
      "Path exists: True\n",
      "\n",
      "ğŸ“ Directory contents:\n",
      "  - metadata.pkl (431,988 bytes)\n",
      "  - faiss_index.bin (28,749,357 bytes)\n",
      "  - embeddings.npy (28,749,440 bytes)\n",
      "  - index_info.json (187 bytes)\n",
      "  - texts.pkl (11,096,569 bytes)\n",
      "\n",
      "âœ… FAISS index loaded: 18717 vectors\n",
      "âœ… Texts loaded: 18717 chunks\n",
      "âœ… Metadata loaded: 18717 entries\n",
      "âœ… Embedding model loaded\n",
      "\n",
      "============================================================\n",
      "ğŸ§ª Testing FAISS search directly:\n",
      "\n",
      "ğŸ” Query: 'automotive startups'\n",
      "  Result 1:\n",
      "    Type: research_paper\n",
      "    Source: automotive_papers_processed.txt\n",
      "    Similarity: 0.5672\n",
      "  Result 2:\n",
      "    Type: startups\n",
      "    Source: autotechinsight_startups_processed.txt\n",
      "    Similarity: 0.5527\n",
      "    ğŸš€ Startups data: âœ“\n",
      "  Result 3:\n",
      "    Type: startups\n",
      "    Source: autotechinsight_startups_processed.txt\n",
      "    Similarity: 0.5459\n",
      "    ğŸš€ Startups data: âœ“\n",
      "\n",
      "ğŸ” Query: 'autonomous driving technology'\n",
      "  Result 1:\n",
      "    Type: research_paper\n",
      "    Source: automotive_papers_processed.txt\n",
      "    Similarity: 0.6073\n",
      "  Result 2:\n",
      "    Type: research_paper\n",
      "    Source: automotive_papers_processed.txt\n",
      "    Similarity: 0.5911\n",
      "  Result 3:\n",
      "    Type: research_paper\n",
      "    Source: automotive_papers_processed.txt\n",
      "    Similarity: 0.5786\n",
      "\n",
      "ğŸ” Query: 'generative AI in automotive'\n",
      "  Result 1:\n",
      "    Type: research_paper\n",
      "    Source: Gen_AI_in_automotive_applications_challenges_and_opportunities_with_a_case_study_on_in-vehicle_experience.txt\n",
      "    Similarity: 0.7408\n",
      "  Result 2:\n",
      "    Type: research_paper\n",
      "    Source: Gen_AI_in_automotive_applications_challenges_and_opportunities_with_a_case_study_on_in-vehicle_experience.txt\n",
      "    Similarity: 0.7183\n",
      "  Result 3:\n",
      "    Type: research_paper\n",
      "    Source: Gen_AI_in_automotive_applications_challenges_and_opportunities_with_a_case_study_on_in-vehicle_experience.txt\n",
      "    Similarity: 0.7065\n",
      "\n",
      "ğŸ” Query: 'electric vehicle innovation'\n",
      "  Result 1:\n",
      "    Type: research_paper\n",
      "    Source: automotive_papers_processed.txt\n",
      "    Similarity: 0.6837\n",
      "  Result 2:\n",
      "    Type: research_paper\n",
      "    Source: automotive_papers_processed.txt\n",
      "    Similarity: 0.6667\n",
      "  Result 3:\n",
      "    Type: research_paper\n",
      "    Source: automotive_papers_processed.txt\n",
      "    Similarity: 0.6167\n",
      "\n",
      "============================================================\n",
      "ğŸ“ˆ Document type distribution:\n",
      "  - research_paper: 12466 chunks\n",
      "  - patents_data: 4198 chunks\n",
      "  - startups: 1366 chunks\n",
      "  - tech_report: 687 chunks\n",
      "\n",
      "============================================================\n",
      "âœ… Direct FAISS test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# SIMPLIFIED DIRECT TEST (no import needed)\n",
    "\n",
    "print(\"ğŸ§ª Direct FAISS index test from Notebook 2\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "from fastembed import TextEmbedding\n",
    "\n",
    "# Path to your FAISS index\n",
    "VECTOR_INDEX_PATH = \"../../04_models/vector_index\"\n",
    "\n",
    "print(f\"Index path: {VECTOR_INDEX_PATH}\")\n",
    "print(f\"Path exists: {os.path.exists(VECTOR_INDEX_PATH)}\")\n",
    "\n",
    "if os.path.exists(VECTOR_INDEX_PATH):\n",
    "    print(f\"\\nğŸ“ Directory contents:\")\n",
    "    for file in os.listdir(VECTOR_INDEX_PATH):\n",
    "        file_path = os.path.join(VECTOR_INDEX_PATH, file)\n",
    "        size = os.path.getsize(file_path)\n",
    "        print(f\"  - {file} ({size:,} bytes)\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Load FAISS index\n",
    "        index_path = os.path.join(VECTOR_INDEX_PATH, \"faiss_index.bin\")\n",
    "        index = faiss.read_index(index_path)\n",
    "        print(f\"\\nâœ… FAISS index loaded: {index.ntotal} vectors\")\n",
    "        \n",
    "        # 2. Load texts\n",
    "        texts_path = os.path.join(VECTOR_INDEX_PATH, \"texts.pkl\")\n",
    "        with open(texts_path, \"rb\") as f:\n",
    "            texts = pickle.load(f)\n",
    "        print(f\"âœ… Texts loaded: {len(texts)} chunks\")\n",
    "        \n",
    "        # 3. Load metadata\n",
    "        metadata_path = os.path.join(VECTOR_INDEX_PATH, \"metadata.pkl\")\n",
    "        with open(metadata_path, \"rb\") as f:\n",
    "            metadatas = pickle.load(f)\n",
    "        print(f\"âœ… Metadata loaded: {len(metadatas)} entries\")\n",
    "        \n",
    "        # 4. Initialize embedding model for queries\n",
    "        model = TextEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        print(\"âœ… Embedding model loaded\")\n",
    "        \n",
    "        # 5. Test queries\n",
    "        test_queries = [\n",
    "            \"automotive startups\",\n",
    "            \"autonomous driving technology\", \n",
    "            \"generative AI in automotive\",\n",
    "            \"electric vehicle innovation\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"ğŸ§ª Testing FAISS search directly:\")\n",
    "        \n",
    "        for query in test_queries:\n",
    "            print(f\"\\nğŸ” Query: '{query}'\")\n",
    "            \n",
    "            # Create query embedding\n",
    "            query_embedding = np.array(\n",
    "                list(model.embed([query]))[0].tolist()\n",
    "            ).astype('float32').reshape(1, -1)\n",
    "            \n",
    "            # Search\n",
    "            k = 3\n",
    "            distances, indices = index.search(query_embedding, k)\n",
    "            \n",
    "            # Show results\n",
    "            found_results = 0\n",
    "            for i, (idx, distance) in enumerate(zip(indices[0], distances[0])):\n",
    "                if idx != -1 and idx < len(texts):\n",
    "                    similarity = 1.0 / (1.0 + distance)\n",
    "                    if similarity > 0.5:  # Threshold\n",
    "                        found_results += 1\n",
    "                        doc_type = metadatas[idx].get('doc_type', 'N/A') if idx < len(metadatas) else 'N/A'\n",
    "                        source = metadatas[idx].get('source', 'N/A') if idx < len(metadatas) else 'N/A'\n",
    "                        \n",
    "                        print(f\"  Result {i+1}:\")\n",
    "                        print(f\"    Type: {doc_type}\")\n",
    "                        print(f\"    Source: {source}\")\n",
    "                        print(f\"    Similarity: {similarity:.4f}\")\n",
    "                        \n",
    "                        # Check if startups\n",
    "                        if doc_type == 'startups':\n",
    "                            print(f\"    ğŸš€ Startups data: âœ“\")\n",
    "            \n",
    "            if found_results == 0:\n",
    "                print(f\"  âŒ No results above threshold 0.5\")\n",
    "        \n",
    "        # Show document distribution\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"ğŸ“ˆ Document type distribution:\")\n",
    "        doc_type_counts = {}\n",
    "        for metadata in metadatas:\n",
    "            doc_type = metadata.get('doc_type', 'unknown')\n",
    "            doc_type_counts[doc_type] = doc_type_counts.get(doc_type, 0) + 1\n",
    "        \n",
    "        for doc_type, count in sorted(doc_type_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  - {doc_type}: {count} chunks\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"âœ… Direct FAISS test completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during direct test: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"âŒ Vector index directory not found!\")\n",
    "    print(\"Make sure you ran the FAISS embedding creation code first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
